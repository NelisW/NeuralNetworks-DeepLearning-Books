% -*- TeX -*- -*- UK -*- -*- Soft -*-


\part{Bayesian Algorithms}

\chapter{Bayesian Overview}
\label{chap:BayesianOverview}

\section{Bayes' or Bayes's?}

Thomas Bayes did the original work. 
Following \cite{MaeveMaddox2019} is should be Bayes' Theorem, on two, perhaps three, accounts, (1) Bayes lived a long time ago, (2) in general vernacular  we say like 'bays' not 'basis', and (3) the UK spelling rules seems to prefer Bayes'.

\section{Frequentist vs. Bayesian Approach}

\newthought{In the frequentist approach} probabilities are defined by the frequency of countable events in the limiting case of repeated measurements (sampling distribution) of very large samples.
In the frequentist view it is meaningless to talk of the probability \textit{of the true value} of something: such a true value is a single fixed value  and cannot have a probability/frequency distribution.  Deviations from the true value are attributable to measurement inaccuracy or noise.
This means that parameters and models cannot have probability distributions, only measurements can. 
For the frequentist (measurement) uncertainty can only be observed by resampling the process, because the true value is always fixed. This resampling is often not possible or never done.\cite{McElreath2015,vanderPlasFreqBayes2014}

\newthought{For Bayesians}, probabilities are fundamentally related to our own knowledge about an event, i.e., counting the numbers of ways things can happen, \textit{but according to additional assumptions beyond the count frequency}. This probability may/will include measurements  of the frequencies of countable events, but is not limited to frequency measurements as the only and fundamental source of information. Bayesian probability codifies our knowledge of the value based on measured observations and/or  prior information\marginnote{The prior is the probability of an event before witnessing any data.}.  There is no single true value (as frequentists  insist), but rather the plausibility of different possibilities. In the Bayesian view, we can talk about the probability that the true value of something falls in a given range. Probability therefore now  has a wider meaning: as the representation of  plausibility, or the degree of certainty about events.  This concept of plausibility applies not only to the measured count frequency, but also applies to models or parameters.
 \cite{McElreath2015,vanderPlasFreqBayes2014}

\newthought{Probability is not unitary.} It will make some readers uncomfortable to suggest that
there is more than one way to define ''probability.'' Aren't mathematical concepts uniquely correct?
They are not. Once you adopt some set of premises, or axioms, everything does follow logically in
mathematical systems. But the axioms are open to debate and interpretation. So not only is there
''Bayesian'' and ''frequentist'' probability, but there are different versions of Bayesian probability even, relying upon different arguments to justify the approach.\cite{McElreath2015}

\section{Concise Overview}

This section is a verbatim extract (with potential minor changes) from Riesenfeld's very nice and tight overview of Bayes' theorem \cite{RichRiesenfeld2011}. 

\subsection{Bayes' Theorem}

Bayes' theorem (also known as Bayes' rule or Bayes' law) is a result in probability theory that relates conditional probabilities. If $A$ and $B$ denote two events,
$P(A|B)$ denotes the conditional probability of $A$ occurring, given that $B$ occurs.
The two conditional probabilities $P(A|B)$ and $P(B|A)$ are in general different.
Bayes theorem gives a relation between $P(A|B)$ and $P(B|A)$.

An important application of Bayes' theorem is that it gives a rule how to
update or revise the strengths of evidence-based beliefs in light of new evidence
a posteriori.

As a formal theorem, Bayes' theorem is valid in all interpretations of probability. However, it plays a central role in the debate around the foundations of
statistics: frequentist and Bayesian interpretations disagree about the kinds of
things to which probabilities should be assigned in applications. Whereas frequentists assign probabilities to random events according to their frequencies of
occurrence or to subsets of populations as proportions of the whole, Bayesians
assign probabilities to propositions that are uncertain. A consequence is that
Bayesians have more frequent occasion to use Bayes' theorem. The articles on
Bayesian probability and frequentist probability discuss these debates at greater
length.


\subsection{Statement of Bayes' theorem}
Bayes' theorem relates the conditional and marginal probabilities of stochastic
events $A$ and $B$:
$$
P(A|B) = \frac{P (B|A) P (A)}{P (B)}.
$$

Each term in Bayes' theorem has a conventional name:
\begin{itemize}
\item $P(A)$ is the prior probability or marginal probability of $A$. It is ``prior'' in
the sense that it does not take into account any information about $B$.
\item $P(A|B)$ is the conditional probability of $A$, given $B$. It is also called the
posterior probability because it is derived from or depends upon the specified value of $B$.
\item $P(B|A)$ is the conditional probability of $B$ given $A$.
\item $P(B)$ is the prior or marginal probability of $B$, and acts as a normalizing
constant.
\end{itemize}

\subsection{Bayes' theorem in terms of likelihood}

Bayes' theorem can also be interpreted in terms of likelihood:
$$P(A|B) \propto L(A|B) P(A).$$

Here $L(A|B)$ is the likelihood of $A$ given fixed $B$. The rule is then an immediate consequence of the relationship $P(B|A) = L(A|B)$. In many contexts
the likelihood function $L$ can be multiplied by a constant factor, so that it is
proportional to, but does not equal the conditional probability $P$.
With this terminology, the theorem may be paraphrased as
$$
\textrm{posterior} = \frac{\textrm{likelihood}\times\textrm{prior}}{\textrm{normalizing constant}}
$$
In words: the posterior probability is proportional to the product of the
prior probability and the likelihood.
In addition, the ratio L(A|B)/P(B) is sometimes called the standardized
likelihood or normalized likelihood, so the theorem may also be paraphrased as
posterior = normalized likelihood Ã— prior.
$$
\textrm{posterior} = \textrm{normalised likelihood}\times\textrm{prior}
$$


\subsection{Derivation from conditional probabilities}

To derive the theorem, we start from the definition of conditional probability.
The probability of event $A$ given event $B$ is
$$P(A|B) = \frac{P (A\cap B)}{P (B)}.$$
Likewise, the probability of event $B$ given event $A$ is
$$P(B|A) = \frac{P (A\cap B)}{P (A)}.$$
Rearranging and combining these two equations, we find
$$P(A|B) P(B) = P(A \cap  B) = P(B|A) P(A).$$
This lemma is sometimes called the product rule for probabilities. Dividing
both sides by $P(B)$, providing that it is non-zero, we obtain Bayes' theorem:
$$P(A|B) = \frac{P (B|A) P (A)}{P (B)}.$$

\subsection{Alternative forms of Bayes' theorem}
Bayes' theorem is often embellished by noting that
$$P(B) = P(A \cap  B) + P(A^C \cap  B) = P(B|A)P(A) + P(B|AC )P(A^C )$$
where $A^C$ is the complementary event of A (often called ``not A''). So the
theorem can be restated as
$$P(A|B) = \frac{P (B|A) P (A)}{P (B|A)P (A)+P (B|AC)P (AC )}.$$
More generally, where $A_i$ forms a partition of the event space,
$$P(A_i|B) = \frac{P (B|Ai) P (Ai)}{\Sigma_j P (B|Aj ) P (A_j )},$$
for any $A_i$ in the partition.
See also the law of total probability.


\subsection{Bayes' theorem in terms of odds and likelihood ratio}
Bayes' theorem can also be written neatly in terms of a likelihood ratio and
odds $O$ as
$$O(A|B) = O(A) \cdot \wedge  (A|B)$$
where
$$O(A|B) = \frac{P (A|B)}{P (A^C |B)}$$
are the odds of $A$ given $B$, and 
$$O(A) = \frac{P (A)}{P (A^C)}$$
are the odds of A by itself,
while 
$$\wedge (A|B) = \frac{L(A|B)}{L(A^C |B)} =\frac{P (B|A)}{P (B|A^C)}$$
is the likelihood ratio.

%\subsection{Bayes' theorem for probability densities}
%There is also a version of Bayes' theorem for continuous distributions. It is somewhat harder to derive, since probability densities, strictly speaking, are not probabilities, so Bayes' theorem has to be established by a limit process; see Papoulis (citation below), Section 7.3 for an elementary derivation. Bayes's theorem for probability densities is formally similar to the theorem for probabilities:
%$$f(x|y) = \frac{f(x,y)}{f(y)} = \frac{f(y|x) f(x)}{f(y)}$$
%and there is an analogous statement of the law of total probability:
%$$f(x|y) = \frac{f(y|x) f(x)}{\int_{-\infty}^{\infty}f(y|x) f(x) dx}.$$
%As in the discrete case, the terms have standard names.
%\begin{itemize}
%\item  $f(x, y)$ is the joint distribution of $X$ and $Y$,
%\item  $f(x---y)$ is the posterior distribution of $X$ given $Y=y$,
%\item $f(y---x) = L(x---y)$ is (as a function of $x$) the likelihood function of $X$ given $Y=y$, and
%\item $f(x)$ and $f(y)$ are the marginal distributions of $X$ and $Y$ respectively, with
%$f(x)$ being the prior distribution of $X$.
%\end{itemize}
%Here we have indulged in a conventional abuse of notation, using $f$ for each
%one of these terms, although each one is really a different function; the functions
%are distinguished by the names of their arguments.

\section{Bayes' theorem for continuous probability densities}

This is taken from \cite{OrloffBloom2014}.

Our hypotheses often take the form a certain parameter has value $\theta$. We will often use the letter $\theta$  to stand for an arbitrary hypothesis. This will leave symbols like $p$, $f$, and $x$ to take there usual meanings as \ac{PMF}, \ac{PDFun}, and data. Also, rather than saying 'the hypothesis that the parameter of interest has value $\theta$' we will simply say the hypothesis $\theta$.

We have two parallel notations for outcomes and probability:
\begin{enumerate}
\item (Big letters) Event $A$, probability function $P(A)$. From hypotheses {\cal H} and data {\cal D} we compute several associated probabilities
$P({\cal H})$, $P({\cal D})$, $P({\cal H}|{\cal D})$, $P({\cal D}|{\cal H})$.

In the coin example we might have ${\cal H}$ = 'the chosen coin has probability 0.6 of heads', ${\cal D}$ = 'the flip was heads', and $P({\cal D}|{\cal H})$=0.6.

\item 
(Little letters) Value $x$, \ac{PMF} $p(x)$ or \ac{PDFun} $f(x)$.
Hypothesis values $\theta$  and data values $x$ both have probabilities or probability
densities:

\begin{center}
\begin{tabular}{cccc}
$p(\theta )$ &$p(x)$ & $p(\theta |x)$ & $p(x|\theta )$\\
$f(\theta )$ &$f(x)$ & $f(\theta |x)$ & $f(x|\theta )$\\
\end{tabular}
\end{center}

In the coin example we might have $\theta$=0.6 and $x$=1, so $p(x|\theta )$=0.6. We might also write $p(x=1|\theta=0.6)$ to emphasize the values of $x$ and $\theta$, but
we will never just write $p(1|0.6)$ because it is unclear which value is $x$ and which is $\theta$.

\end{enumerate}

These notations are related by $P(X = x)= p(x)$, where $x$ is a value the discrete random
variable $X$ and '$X$=$x$' is the corresponding event.
We will mostly use the small letter notation involving \ac{PMF}s  and  \ac{PDFun}s. Hypotheses will usually be parameters represented by Greek letters $(\theta, \lambda, \mu, \cdots)$ while data values will usually be represented by English letters $(x, x_i, y, \cdots)$.

\newthought{The statement of Bayes' theorem} for continuous \ac{PDFun}s is essentially identical to the statement
for \ac{PMF}s. We state it including $d\theta$ so we have genuine probabilities:

\textbf{Theorem}. Bayes' Theorem. Use the same assumptions as in the law of total probability,
i.e. $\theta$  is a continuous parameter with pdf $f(\theta )$ and range $[a, b]$; $x$ is random discrete data; together they have likelihood $p(x|\theta )$. With these assumptions:
\begin{equation}
f(\theta | x) d \theta=\frac{p(x | \theta) f(\theta) d \theta}{p(x)}=\frac{p(x | \theta) f(\theta) d \theta}{\int_{a}^{b} p(x | \theta) f(\theta) d \theta}.\label{eq:bayesp05c01-01}
\end{equation}
\textbf{Proof}. Since this is a statement about probabilities it is just the usual statement of Bayes'
theorem. This is important enough to warrant spelling it out in words: Let $\Theta$  be the random
variable that produces the value $\theta$ . Consider the events
\begin{itemize}
\item 
H = '$\Theta$  is in an interval of width $d\theta$  around the value $\theta$', and 
\item 
$D$ = 'the value of the data is $x$'.
\end{itemize}
Then $P(H) = f(\theta ) d\theta$, $P(D) = p(x)$, and $P(D|H) = p(x|\theta )$. Now our usual form of Bayes' theorem becomes
\begin{equation}
f(\theta | x) d \theta=P(H | D)=\frac{P(D | H) P(H)}{P(D)}=\frac{p(x | \theta) f(\theta) d \theta}{p(x)}.\label{eq:bayesp05c01-02}
\end{equation}
Looking at the first and last terms in this equation we see the new form of Bayes' theorem.

Finally, we firmly believe that it is more conducive to careful thinking about probability
to keep the factor of $d\theta$ in the statement of Bayes' theorem. But because it appears in the
numerator on both sides of Equation~\ref{eq:bayesp05c01-01}. Many people drop the $d\theta$ and write Bayes' theorem in terms of densities as
\begin{equation}
f(\theta | x)=\frac{p(x | \theta) f(\theta)}{p(x)}=\frac{p(x | \theta) f(\theta)}{\int_{a}^{b} p(x | \theta) f(\theta) d \theta}.\label{eq:bayesp05c01-03}
\end{equation}

\section{Bayesian Glossary}
\label{sec:BayesianGlossary}

This material is taken from \cite{RavinKumarBayesianGlossary2019}, several significant refinements and corrections.

When reading Bayesian texts, or listening to lectures, terms like ''posterior'' or ''data'' are used, but often without explanation leaving the audience confused. I also find that even after learning the concepts once, its easy to mix them up. To that end I put together this glossary to serve as a quick reference to the terms, with examples. The writing is colloquial, for precise definitions I recommend the following references:
\begin{enumerate}
\item Bayesian Analysis with Python \cite{martin2018}. 
\item Bayesian Data Analysis \cite{gelmanbda04}.
\item Statistical Rethinking \cite{McElreath2015}. For further background I highly suggest purchasing his book and watching his lectures on YouTube.
\end{enumerate}

\subsection{Motivating Example: Proportion of water on a globe}

We will be using this example from Richard McElreath \cite{McElreath2015} to help define all terms. In his example, we want to know the proportion of water on a given globe. We make estimations by independent tosses of a globe, catching it, and seeing whether our right index finger is on water (W, probability $p$) or Land (L, probability $1-p$).  For this example assume the results to be \lstinline{W L W W W L W L W}.

We will be calculating some values, with the code shown in line with the text.
\begin{lstlisting}
    import numpy as np
    from scipy import stats
    import arviz as az
    import pymc3 as pm
    import matplotlib.pyplot as plt
    import seaborn as sns
    import pandas as pd
\end{lstlisting}

\subsection{Glossary}

\newthought{Probability Mass Function (PMF)}
 is a function that gives the probability that a discrete random variable is exactly equal to some value. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete.

A probability mass function differs from a probability density function (PDF) in that the latter is associated with continuous rather than discrete random variables; the values of the probability density function are not probabilities as such: a PDF must be integrated over an interval to yield a probability.

\newthought{Data} in Bayesian analysis is the fixed truth of what has been observed. There is no uncertainty or probability. In our example the globe was tossed, the sequence of events observed is \lstinline{Water, Land, Water Water Water, Land, Water, Land, Water}.

\newthought{A Model} is a representation of some other thing.  In Bayesian analysis a model is a representation of the data generating process made of out mathematical distributions. Below is a model of the globe tossing example, written in pseudo mathematical notation.

\begin{lstlisting}
proportion_of_water=p=Uniform(0,1)
count_water_obs=Binomial(p,number_of_tosses)
\end{lstlisting}
It is important to know that models, unlike data, are not fixed and are completely human made. Two models can exist at the same time. For example, here is another model of the globe tosses:
\begin{lstlisting}
lamda=Uniform(0,1)
count_water_obs=Poisson(lamda)
\end{lstlisting}
Neither model is correct, nor is either model wrong! Both models are only a representation of the globe tossing, and justifying model choice is a core activity of Bayesian analysis.

\newthought{Bayes Theorem} is the probability event, taking into account past events that provide information about the event. The most generic formulation is 

\begin{equation}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{equation}
A representation that I find more intuitive however is this one 

\begin{equation}
    P(\textrm{parameters}|\textrm{data}) = 
    \frac{P(\textrm{data}|\textrm{parameters})P(\textrm{parameters})} {P(\textrm{data})}
\end{equation}

Sometimes Bayes Theorem is written as shown below \cite{WikiPediaLikelihoodfunction2019}. This perhaps is the most ''technically correct'' because it differentiates between likelihood and probability. (See likelihood and probability glossary terms below)
\begin{equation}
    P(\textrm{parameters}|\textrm{data}) = 
    \frac{{\cal L}(\textrm{data}|\textrm{parameters})P(\textrm{parameters})} {P(\textrm{data})}
\end{equation}

\newthought{Bayesian inference} is a way to update probabilities by using Bayes formula. In other words Inference is solving for the posterior, or left hand side, of Bayes formula. Inference can be performed numerous ways, a partial list includes
\begin{itemize}
\item  Direct solution with point probabilities
\item  Conjugate prior formulas
\item  Grid Search
\item  Quadratic Approximation
\item  Markov Chain Monte Carlo
\item  Variational Inference
\end{itemize}
Different methods of inference each have their own advantages and disadvantages. It is important to note that his also is a human choice, and different books, guides, and tutorials may show different ways of solving for the posterior.

\newthought{The prior} is the probability of an event before witnessing any data.

For our globe tossing example before tossing the globe and making any observations, how much of the globe do you think covered in water? Some people would say 70\% because they took a geography class in a prior life. Others would make a reaction similar to this emoji, \includegraphics[height=4mm]{emoji-shrug}, indicating they are equally unsure about all possibilities.

The choice of priors is a choice the Bayesian modeler must make. There is no fundamental truth. But luckily there is some help, for example the Stan Devs have a great tutorial providing recommendations on choice of priors \cite{StanwikiPriors2019}:
\begin{itemize}
\item Flat prior;
\item Super-vague but proper prior: normal(0, 1e6);
\item Weakly informative prior, very weak: normal(0, 10);
\item Generic weakly informative prior: normal(0, 1);
\item Specific informative prior: normal(0.4, 0.2) or whatever. Sometimes this can be expressed as a scaling followed by a generic prior: \\$\theta = 0.4 + 0.2*z$, $z\sim$ normal(0, 1).
\end{itemize}


\begin{marginfigure}
    \includegraphics{bayesglos01}
    \caption{Prior probability of \lstinline{Water} is 0.1}
    \end{marginfigure}


\begin{lstlisting}
    # Create a prior in numpy
    possible_proportion_of_water = np.linspace(0,1,100)
    probability_of_possible = np.repeat(.1, 100)
    
    fig,ax=plt.subplots()
    ax.plot(possible_proportion_of_water, probability_of_possible)
    fig.suptitle("Prior probability of Water")
    plt.savefig('bayesglos01.pdf')
\end{lstlisting}

\newthought{The likelihood} is the probability of data, given a model and parameters.  
Assuming a binomial model, the likelihood of $W$ water events out of $N$ tosses, with proportion of \lstinline{Water} of $p$ is given by
\begin{equation}
P(W|N,p) = \frac{N!}{W!(N-W!)}p^W(1-p)^{N-W}
\end{equation}
The likelihood of $W$=6 \lstinline{Water} events given $N$=9 tosses, with a proportion of water of $p$=0.5 can be calculated
\begin{lstlisting}
count_of_water_observation = 6
count_of_tosses = 9
likelihood = stats.binom.pmf(k=count_of_water_observation,
                             n=count_of_tosses, p=.5)
print(f'likelihood={likelihood}')    
>>> likelihood=0.16406250000000006
\end{lstlisting}
The likelihood of $W$=6 \lstinline{Water} events given $N$=9 tosses, for all possible values of $p$ (between 0 and 1) can be calculated as shown in the graph.
\begin{marginfigure}
    \includegraphics{bayesglos02}
    \caption{Likelihood of 6 out of 9 tosses being water for $0\leq p\leq 1$}
    \end{marginfigure}
\begin{lstlisting}
likelihood = stats.binom.pmf(k=count_of_water_observation, 
n=count_of_tosses,
p=possible_proportion_of_water)
fig,ax=plt.subplots()
ax.plot(possible_proportion_of_water, likelihood)
fig.suptitle("Likelihood of 6 Water Observations given 9 tosses \n \
over all possible Proportions of Water")
plt.savefig('bayesglos02.pdf')
\end{lstlisting}

\textbf{Note:} Likelihood is not a probability distribution, meaning that the total area of the curve is not equal to 1. This is why the syntax 
${\cal L}(\textrm{data}|\textrm{parameters})$
can be a bit more clear than $P(\textrm{data}|\textrm{parameters})$


\newthought{Marginal Likelihood of Evidence/Evidence/Average Likelihood}.
The ''Bayesian way'' to compare competing models $\alpha_i$ is to compute the marginal likelihood of each model $p(\textrm{data}|\alpha_1)$, i.e. the probability of the observed data $\textrm{data}$ given the $\alpha_i$ model \cite{PyMC3marglikeli2019}.

The marginal likelihood, also known as the evidence, or model evidence, is the denominator of the Bayes equation. Its only role is to guarantee that the posterior is a valid probability by making its area sum to 1. It is a likelihood function in which some parameter variables have been marginalized (integrated or summed out). 
If $\theta$ is the parameter being marginalised 
\begin{equation}
P(\textrm{data}|\alpha) = \int_\theta p(\textrm{data}|\theta)P(\theta|\alpha)d\theta
\end{equation}
Integrate the data given a parameter, weighted by the parameter value.

We can see the normalising effect if we write Bayes' theorem and make explicit the fact that all inferences are model-dependant.
\begin{equation*}
    P(\textrm{parameters}|\textrm{data},\alpha_i) = 
    \frac{P(\textrm{data}|\textrm{parameters},\alpha_i)P(\textrm{parameters}|\alpha_i} {P(\textrm{data}|\alpha_i)}
\end{equation*}

In most inference methods however this term is readily ignored, and ends up being derived out of the inference. The reason being its non trivial to calculate analytically, and very hard to simulate in multi-dimensional problems.


From \cite{Quoramarginallikelihoodandposterior2019}:
The marginal likelihood's only role is to guarantee that the posterior is a valid probability by making its area sum to 1.
Therefore, its only effect in the posterior is that it scales it up or down, but the shape of the posterior does not change.
Here you have the Bayes formula to get a posterior distribution. I express it in different ways in case some of the expressions is more familiar to you:
\begin{eqnarray}
\overbrace{p(\boldsymbol{\theta} | \mathbf{X})}^{\text { posterior }}
&=&
\frac{\overbrace{p(\mathbf{X} | \boldsymbol{\theta}) p(\boldsymbol{\theta} | \alpha)}^{\text { likelihood prior }}}
{
    \underbrace{ \int_{\boldsymbol{\theta}} p(\mathbf{X} | \boldsymbol{\theta}) p(\boldsymbol{\theta} | \alpha)}_{\text { marginal likelihood }}
    }\nonumber\\
&=& \frac{
    \overbrace{p(\mathbf{X} | \boldsymbol{\theta})}^{\text { likelihood}}
    \overbrace{p(\boldsymbol{\theta} | \alpha)}^{\text {prior }}
    }{
        \underbrace{p(\mathbf{X} | \alpha)}_{\text { marginal likelihood }}
        }\nonumber\\
&=&\frac{\overbrace{p(\mathbf{X}, \boldsymbol{\theta})}^{\text { joint probability }}}
{
    \underbrace{p(\mathbf{X} | \alpha)}_{\textrm{marginal likelihood}}
    }
\nonumber\\
&=&K p(\mathbf{X}, \boldsymbol{\theta}) \nonumber\\
&\propto& p(\mathbf{X}, \boldsymbol{\theta})\nonumber
\end{eqnarray}
where the $\alpha$ parameter that controls the prior is often implicit and we just write things like $p(\boldsymbol{\theta})$  or $p(\mathbf{X})$. As you can see, the marginal likelihood (marginal means that we marginalised, integrated, the variable $\boldsymbol{\theta}$) is different from the posterior. It is only one of the three elements needed to get the exact posterior.



Therefore the  Bayes formula often written where the denominator is removed, and a proportional to is shown rather than an equal:
\begin{equation*}
    P(\textrm{parameters}|\textrm{data},\alpha_i) \propto
    P(\textrm{data}|\textrm{parameters},\alpha_i)P(\textrm{parameters}|\alpha_i)
\end{equation*}
For our globe tossing example we are able to calculate the Marginal Likelihood because we are using a Grid Search Inference method can use the formulation below.
\begin{lstlisting}
    # Marginal Likelihood in Grid Search formulation
    numerator  = possible_proportion_of_water * likelihood
    denominator = sum(numerator)
    denominator
    
    6.300000720691165
    
\end{lstlisting}

\newthought{The posterior, or posterior probability}, is the probability of model parameters after incorporating the data. In the our water example, prior to the experiment, we were equally sure, (or unsure) what the proportion of water on the planet was. After using our model in conjunction with the data, we the posterior distribution reflects our certainty in the parameters.

Note that the posterior distribution is a distribution of possible model parameters and not a distribution of data.

This example is very nicely explained in McElreath's book \cite[Chapter 2]{McElreath2015} and video series.

\begin{lstlisting}
posterior = numerator/denominator
fig, axes =plt.subplots(1,2, figsize=(12,5))
axes[1].plot(possible_proportion_of_water, posterior)
axes[1].set_title("Posterior Probability of Proportion of Water")
axes[0].plot(possible_proportion_of_water, probability_of_possible) 
axes[0].set_title("Prior Probability of Proportion of Water")
plt.savefig('bayesglos03.pdf')
\end{lstlisting}
\begin{figure*}[h]
    \includegraphics[width=0.56\textwidth]{bayesglos03}
    \caption{Prior and posterior probability of water}
    \end{figure*}

\newthought{Posterior Predictive} 
An advantage of Bayesian models is being able to calculate data after the model has been updated. The simulation of data, taking into account observed data, is called the posterior predictive. The mathematical formulation is as follows.
\begin{equation}
    \operatorname{Pr}\left(y^{\prime} | y\right)=\int \operatorname{Pr}\left(y^{\prime} | \theta\right) \operatorname{Pr}(\theta | y) d \theta
    \end{equation}
We can simulate the posterior predictive distribution using python. 
\begin{lstlisting}
posterior_samples = np.random.choice(possible_proportion_of_water, p=posterior, size=1000, replace=True)
posterior_predictive = stats.binom.rvs(n=9, p=posterior_samples)
counts = np.bincount(posterior_predictive)

fig, ax = plt.subplots()
ax.bar(x=np.arange(counts.shape[0]), height=np.bincount(posterior_predictive))
ax.set_xlabel("Count of Water Observations out of 9 globe tosses")
ax.set_ylabel("Number of simulations with count")
plt.savefig('bayesglos04.pdf')
\end{lstlisting}

\begin{marginfigure}
    \includegraphics{bayesglos04}
    \caption{Posterior Predictive}
    \end{marginfigure}

It is important to note that this distribution is not probability. It is in the units of data, in this case it is the distribution of ''counts of water observations for 9 globe tosses''. The distribution highlights the uncertainty in the globe outcome given the 9 data points that were originally observed.

\newthought{Prior Predictive Distribution}. 
Prior Predictive models are similar to posterior predictive distributions, except that you use samples from the prior distribution of parameters, not from the posterior distribution of samples.
\begin{equation}
    p(y)=\int_{\theta} p(\theta) p(y | \theta) \mathrm{d} \theta
    \end{equation}
Prior Predictive distributions are useful for checking if your model seems to be outputting reasonable data, for example in our globe tossing experiment, if we were seeing negative counts for ''Number of water observations'', that impossibility would suggest that the model is not appropriate for the data
\begin{lstlisting}
    prior_samples = stats.uniform(0,1).rvs(1000)
    prior_predictive_samples = stats.binom.rvs(n=9, p=prior_samples)
    counts = np.bincount(prior_predictive_samples)
    
    fig, ax = plt.subplots()
    ax.bar(x=np.arange(counts.shape[0]), height=np.bincount(prior_predictive_samples))
    ax.set_xlabel("Count of Water Observations out of 9 globe tosses")
    ax.set_ylabel("Number of simulations with count")
    plt.savefig('bayesglos05.pdf')
\end{lstlisting}

\begin{marginfigure}
    \includegraphics{bayesglos05}
    \caption{Posterior Predictive}
    \end{marginfigure}

\newthought{Forward Sampling} is the same as Prior Predictive except that it includes the prior samples. Whereas Prior Predictive is just the distribution of simulated data, Forward Sampling is that in addition to the sampled parameters from the posterior.

\newthought{Hierarchical modelling} is a mechanism in Bayesian models where you ''tell'' the model that data points may share similarity. The hierarchy words implies multiple ''levels'' of effect.

For example assume you're trying to estimate the height of an individual. Your data tells you the gender and which family the individual comes from. The relation between height and family is not constant, it's not as if coming from one family guarantees all members of a family will be double the height of another for example. But it would be unwise to assume that your samples are independent either, family genetics is well correlated with final height. But with hierarchical models allow us to split the difference, by allowing the model to ''say'' a families heights come from a taller distribution, than from another family with shorter family members.

The Radon Model \cite{PyMC3multilevel2019} provides a rigorous explanation accompanied by a great visual explainer

\newthought{Hierarchical Funnel}

% Hierarchical funnels are a particular parameter space topology that makes it hard for particular Inference Engines to explore the whole space. Basically what ends up happening is this, where the Bayesian sampler doesn't quite do the thing you want, but instead of your donation coin taking a long time to fall a short vertical distance, your sampler can't traverse the entire space efficiently.

Michael Betancourt wrote a full explanation on the Stan website \cite{MichaelBetancourtdivergence2017}. The tutorial has also been rewritten in PyMC3 \cite{PyMC3Divergences2019}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage
%\TBC{To be completed from here onwards}
%
%\lstinline{https://frnsys.com/ai_notes/foundations/bayesian_statistics.html}
%
%\lstinline{https://frnsys.com/ai_notes/machine_learning/bayesian_learning.html}
%
%
%\section{Defining the Problem}
%
%\newthought{Allen Downey's 2019 SciPy tutorial} lists this approach to defining the problem in Bayes terms:
%
%\begin{enumerate}
%\item That are the parameters?  What is my system? What am I trying to estimate?
%What is the quantity that I want to know?
%
%\item What are the hypotheses?
%What are the possible values for those parameters?
%
%
%\item What is the prior?
%What do I know about the domain before I see the outcome of the experiment?
%
%
%\item What is the likelihood function?
%If someone tells me the hypothetical parameters, can I compute the probability of the data?
%What it the probability of my data if I knew what the parameters were?
%
%\end{enumerate} 