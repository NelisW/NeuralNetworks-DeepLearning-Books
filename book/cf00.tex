% -*- TeX -*- -*- UK -*- -*- Soft -*-

% Front matter
\frontmatter

% r.1 blank page
\blankpage


% r.3 full title page
\maketitle


% v.4 copyright page
\newpage
\begin{fullwidth}
~\vfill
\thispagestyle{empty}
\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}
Copyright \copyright\ \the\year\ \thanklessauthor, as indicated in the text

%\par\smallcaps{Published by \thanklesspublisher}

\par\smallcaps{https://github.com/NelisW/NeuralNetworks-DeepLearning-Notes}

\par
This work is licensed under a Creative Commons Attribution-
NonCommercial 4.0 Unported License. This means you're free to
copy, share, and build on this book, but not to sell it. 
You may obtain a copy
of the License at \url{https://creativecommons.org/licenses/by-nc/4.0/}. Unless
required by applicable law or agreed to in writing, software distributed
under the License is distributed on an \smallcaps{``AS IS'' BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND}, either express or implied. See the
License for the specific language governing permissions and limitations
under the License.\index{license}
If you're interested in commercial use, please contact both the authors.

\par\textit{Current printing, \monthyear}
\end{fullwidth}

% r.5 contents
\tableofcontents

\listoffigures

%\listoftables

\input{abbrev.tex}


%%
% Start the main matter (normal chapters)
\mainmatter

\chapter*{Introduction}


\newthought{There are several different approaches} to learning about and mastering \ac{ML}.  

The top-down approach provides pre-packaged solutions, for which you just prepare your data and use the canned algorithms. fastai \cite{fastai2019} is an excellent example of the top-down approach and recommended to get solutions quickly and relatively painlessly. There are also many examples and tutorials available on the Internet that can be modified to suit your problem's needs.  If the inner workings of these tools are not important to you, go this route. 

The academic approach provides a rigourous and deep theoretical base from which to develop solutions.  This is where the real progress is made, while also being solution driven, the academic explore new ideas and push the boundaries. To prepare yourself for this approach do a university course or work through some of the several good books with strong mathematics  \cite{geron2017handson,Webb2002statpatn,Michie94,theodoridis2003,Duda2001,Bishop1995,Bishop2006,Goodfellow2016}. 
Going this route requires hard work and testing before solutions are found. Furthermore you will have at least a good understanding of how things work. Best of all, you can solve new and previously unsolved problems.

The purpose with this book is to document is not to do either of the above, for those purposes there are better resources available elsewhere.  My approach here is from an informal, almost hobbyist, perspective as I explore and learn about \ac{ML}. The focus here is on exploring the concepts and explaining these concepts to gain an intuitive but practical understanding\marginnote{Nielsen's online book \cite{Nielsen2015} (repeated here as Part I) is excellent to open understanding for the underlying concepts.}.  If you want the terse academic treatment, this book is not for you. This document also does not attempt to add any new knowledge to the field, nor to provide packaged solutions.

There is a huge body of knowledge on the internet (blogs, YouTube and academic papers). Apart from the academic papers, most of these sources are of introductory or review nature. YouTube is littered with garbage,  but there is some good material also, offering \ac{MOOC} or university course material. Some large companies also promote their products with good YouTube video and blog material. Filter and read selectively.

The book has three distinctly separate parts: it contains Michael Nielsen's book as Part I, my own notes in Part II, and some more information from diverse sources in Part III.  The material in Part I is presented with minimal change so as to retain Michael's excellent review content.  The material in Part II is part original and part a rehash of other people's work. The material in Part III is mostly a rehash of other people's work.

Any publication is bound to be outdated on the date of publication, especially in the very fast moving field of \ac{ML}. Companies are updating products regularly with breaking changes and new theory and algorithms are becoming available daily. We try to keep to the stable fundamentals here, but even so there could be better ways than described here.

\section*{Terminology}

\begin{marginfigure}
\includegraphics{chapterf00-01}
\end{marginfigure}

The newcomer can easily be confused by the terminology used in the field.  There is some differences in the details of different definitions, but the broad consensus can be depicted as fields of expertise within fields of expertise, often drawn as a Venn diagram as shown here.

\newthought{Machine learning} is defined as\cite{DanielFaggella2019} ``Machine Learning is the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.''

\ac{AI} can be broadly defined as the simulation of human intelligence by machines, including computers. These \ac{AI} processes include learning, reasoning and self-correction. Work in this area started as early as the 1940s. \ac{ML}\cite{WikiPediaMachineLearning2019,DanielFaggella2019}, is normally defined as the application of \ac{AI} to learn from data without explicitly programming the learned behaviour. \ac{ML} therefore focuses on the development of programs that can access data and learn new behaviour from the data, by themselves.  Early ML work started in the 1980s. 

The two main topics in this book are \ac{EAlg} and \ac{ANN}. These two approaches to \ac{AI} are fundamentally different in objective and procedure \cite{KamilTomsik2017,redditGAANN2017,QuoraGAANN2016}:
\begin{itemize}
\item
You use \textit{evolutionary algorithm if you yet don't know the answer} but you are able to somehow rate candidates and provide meaningful mutations.  You have some environment and want to optimize some value/parameter to get a good fitness. The core point is you don't need data beforehand. The data appears during the training process. For evolutionary algorithms you don't have a gradient. You try to randomly perturb parameters and hope to find a better set of parameters. For neural networks you already know in which direction you need to tweak your parameter as the gradient is known.

Genetic algorithm is a randomized numerical optimization method. Which means it gives the parameters which optimize a particular function. Genetic algorithm start with several randomly chosen parameters and retain the set of parameters which give a lower loss. Solutions are combined (crossover) and random mutation giving a biological analogy.

\item
\textit{Neural network is great if you already have answers (and inputs)} and you want to "train the computer" so it can "guess" the answers for unknown inputs. Also, you don't have to think a lot about the problem, the network will figure it out by itself.
A neural network gets trained on available data. Typically this process is much more efficient because a gradient is already available.

From an optimization/mathematical point of view, Neural nets is a way to describe a trainable function which is known to perform very well on a wide variety of tasks. The biological analogy from Neural Nets is the functioning of brains. Usually gradient based techniques are used to train the neural network, which ofcourse has no biological motivation.
\end{itemize}


Arguably, the neural networks level or subset does not belong to the figure shown here, but this technology plays such a key role that perhaps it earns to be explicitly shown.  Neural nets is a computing technique loosely modelled on the human brain, that can  recognise patterns.  These neural nets are a dominant part of the bigger machine learning field. Neural nets attracted much attention in the 1980's and 1990s, although it was used extensively, the application of this technology was limited by computing power.  In particular, the pattern information had to be carefully prepared for the neural net, which sometimes required very special skills. Practical application of neural nets were limited to three-layer neural nets, which could be trained by the then-available algorithms and computers.  \ac{DLea}\cite{WikiPediaDeepLearning2019}, which required more than three layer neural nets, with deeply hidden layers (hence the term deep learning) was defined and understood already in the 1980s.  The algorithms and computers could not solve the deep learning problems at the time.   Around 2006 new algorithms were was proposed that opened up the deep learning neural nets to practical use.
Deep learning is therefore a special application of neural nets, which is a part of machine learning, which in turn is part of artificial intelligence.




\section*{Historic Perspective}

In his excellent blog post \cite{Vazquez2018} Favio V\'{a}zquez gives a brief introduction to the history of \ac{DL} with a very nice historic time line (Figure~\ref{fig:deeplearningtimeline}).

\begin{figure*}[tph]
\includegraphics[width=\textwidth]{deeplearningtimeline}
\caption{Favio V\'{a}zquez's \ac{DL} time line}
\label{fig:deeplearningtimeline}
\end{figure*}



%\FloatBarrier

\section*{The Plan}

My present motivated plan to learn more about \ac{ML} is as shown below. This is the third plan since I started, which demonstrates the volatility of the \ac{ML} world. What seems like a good idea today is in insanely poor choice six months later.

\begin{enumerate}
\item \marginnote{A study \cite{Ericsson93therole} found that full mastery requires long and consistent practice, but another study \cite{Macnamara2014} found that practice alone is not a guarantee for success.} Select the technology/product with utmost care.  Provided you have the ability, it takes ten years or ten thousand hours to fully master a topic. How many ten-year cycles can you afford in your lifetime?

\item \marginnote{Even staying with large companies is risky. Google is notorious for dropping services. Do you remember the MFC technology from Microsoft? It is interesting that Qt, HTML, Python and JavaScript are still around after all these years.} You want to invest in a technology with a long shelf life (probably not possible in \ac{ML}). Stay with mainstream and large projects with a wide following. Stay away from small or risky projects, time spent there is time wasted. 
    
\item I want to stay on the Windows platform this is where all my tools are (I do not have a Linux boot PC at present).  This requirement severely limits my options (see below). Linux is miles ahead here, most of the serious work is done on Linux.
    
\item Stay local, not in the cloud.   If you have large models with large data sets, using the cloud services is far better, but I have no funding to pay for cloud services at this time.

\item Stay with open source projects with large following and a good Internet support base. The bigger the user base, the better you protect your investment in terms of support and long term survivability. Services such as StackOverflow provide more useful support than does commercial companies, simply because of the large support base. People often denigrate the value of YouTube, but there are also some seriously useful tutorials and support information there. 

\item Steps:
\begin{enumerate}
\item Start by working through the ``Machine Learning for Humans'' introductory text by Maini \cite{Maini2017} to paint the big picture.
\item Work through Michael Nielsen's excellent introductory online book \cite{Nielsen2015}. His book is included as Part~I of this document.  The purpose with the book is to provide understanding at a conceptual level, not as a rigourous academic treatise; which is what I need to start of with. \marginnote{In the online version of the book, Nielsen provides interactive JavaScript tools which do not transfer well to a PDF document.  As an alternative, I coded some of the interactive functionality in Python notebooks, which are available on the GitHub page \cite{willersNNDLgithub2019}. I also sometimes add static screen shots from Nielsen's online tools.}
\item Start with Scikit Learn, simply because it is easily installable, does not require a \ac{GPU} software installation, has a large example base and is reasonably well documented.
\item Invest in learning TensorFlow \cite{TensorFlow2Alpha2019}.  I previously considered PyTorch \cite{paszkePyTorch2017,PyTorch2019} and fastai \cite{fastai2019}, but found it difficult to do a GPU install on Windows.  One of the objections against TensorFlow~1 was its awkward \lstinline{sessions}  construction, but this falls away with TensorFlow~2.  Google also integrated Keras \cite{cholletkeras2015,cholletkerasio2015} into TensorFlow~2 as a native higher-level framework.
    Installation of GPU TensorFlow~2 is still not simple for Windows, but I will start with the \ac{CPU} version. GPU functionality is readily available on Linux or on Google's online CoLaboratory \cite{GoogleCoLaboratory2019}.
\item Work through G\'{e}ron's book \cite{geron2017handson} \textit{Hands-on machine learning with Scikit-Learn and TensorFlow}.  The first part of the book uses Scikit Learn to establish a good understanding and then moves on to using the TensorFlow tools.
    The book receives good reviews and is presently under revision for a second edition, covering TensorFlow~2.
\item I want to document my learning process in Part~II of this book, and add insights from other authors into Part~III of this book. This will come as time availability allows.
\item For in-depth backup and if I ever need this, I will consult the academic books \cite{geron2017handson,Webb2002statpatn,Michie94,theodoridis2003,Duda2001,Bishop1995,Bishop2006,Goodfellow2016}, but for now, my interest is still at an elementary level.
\end{enumerate}
\item All of the above will be done at a leisurely pace outside of working hours and jointly with my life partner and fellow traveller Riana.
\end{enumerate}


\section*{Attribution}

\begin{enumerate}
\item 
The entirety of Part I is taken from Michael Nielsen's online book\cite{Nielsen2015}. Some minor changes were made to the text.

\item
The stylistic neuron on the Part header pages were adapted from \cite{Erler2004}. 

\item
Some of the neural diagrams  are created with the \lstinline{neural} TikZ module for \LaTeX{}\cite{cowan2019} by Mark Cowan.
\end{enumerate}

\section*{Resources}

The Internet is littred with information.  This is just a list of some of the high quality content.

\begin{enumerate}
\item Discover Artificial Intelligence \cite{AnkitGupta2019}.
\item How Deep Neural Networks Work  \cite{BrandonRohrer2019}.
\end{enumerate} 