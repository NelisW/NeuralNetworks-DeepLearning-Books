% -*- TeX -*- -*- UK -*- -*- Soft -*-

% Front matter
\frontmatter

% r.1 blank page
\blankpage


% r.3 full title page
\maketitle


% v.4 copyright page
\newpage
\begin{fullwidth}
~\vfill
\thispagestyle{empty}
\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}
Copyright \copyright\ \the\year\ \thanklessauthor, as indicated in the text

%\par\smallcaps{Published by \thanklesspublisher}

\par\smallcaps{https://github.com/NelisW/NeuralNetworks-DeepLearning-Notes}

\par
This work is licensed under a Creative Commons Attribution-
NonCommercial 4.0 Unported License. This means you’re free to
copy, share, and build on this book, but not to sell it. 
You may obtain a copy
of the License at \url{https://creativecommons.org/licenses/by-nc/4.0/}. Unless
required by applicable law or agreed to in writing, software distributed
under the License is distributed on an \smallcaps{``AS IS'' BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND}, either express or implied. See the
License for the specific language governing permissions and limitations
under the License.\index{license}
If you’re interested in commercial use, please contact both the authors.

\par\textit{Current printing, \monthyear}
\end{fullwidth}

% r.5 contents
\tableofcontents

\listoffigures

%\listoftables

\input{abbrev.tex}


%%
% Start the main matter (normal chapters)
\mainmatter

\chapter*{Introduction}

The purpose with this book is to document some of my learning about \ac{ML}.
This document does not attempt to add any new knowledge to the field, there are several other resources with strong mathematics available for this purpose \cite{geron2017handson,Webb2002statpatn,Michie94,theodoridis2003,Duda2001,Bishop1995,Bishop2006,Goodfellow2016}. There is also a huge body of knowledge on the internet, most of it of introductory or review nature, but very useful to gain a general (non-mathematical) understanding.
The focus here is on exploring the concepts and explaining these concepts to gain an intuitive but practical understanding.

The book has three distinctly separate parts: it contains Michael Nielsen's book as Part I, my own notes in Part II, and some more information from diverse sources in Part III.  The material in Part I is presented with minimal change so as to retain Michael's excellent review content.  The material in Part II is part original and part a rehash of other people's work. The material in Part III is mostly a rehash of other people's work.

\section*{Terminology}

\begin{marginfigure}
\includegraphics{chapterf00-01}
\end{marginfigure}

The newcomer can easily be confused by the terminology used in the field.  There is some differences in the details of different definitions, but the broad consensus can be depicted as fields of expertise within fields of expertise, often drawn as a Venn diagram as shown here.

\ac{AI} can be broadly defined as the simulation of human intelligence by machines, including computers. These \ac{AI} processes include learning, reasoning and self-correction. Work in this area started as early as the 1940s. \ac{ML}\cite{WikiPediaMachineLearning2019,DanielFaggella2019}, is normally defined as the application of \ac{AI} to learn from data without explicitly programming the learned behaviour. \ac{ML} therefore focuses on the development of programs that can access data and learn new behaviour from the data, by themselves.  Early ML work started in the 1980s. Arguably the neural networks subset does not belong to the figure, but this technology plays such a key role that perhaps it should earn to be explicitly shown.  Neural nets is a computing technique loosely modelled on the human brain, that can  recognise patterns.  These neural nets are a dominant part of the bigger machine learning field. Neural nets attracted much attention in the 1980's and 1990s, although it was used extensively, the application of this technology was limited by computing power.  In particular, the pattern information had to be carefully prepared for the neural net, which sometimes required very special skills. Practical application of neural nets were limited to three-layer neural nets, which could be trained by the then-available algorithms and computers.  \ac{DLea}\cite{WikiPediaDeepLearning2019}, which required more than three layer neural nets, with deeply hidden layers (hence the term deep learning) was defined and understood already in the 1980s.  The algorithms and computers could not solve the deep learning problems at the time.   Around 2006 new algorithms were was proposed that opened up the deep learning neural nets to practical use.
Deep learning is therefore a special application of neural nets, which is a part of machine learning, which in turn is part of artificial intelligence.

\newthought{Machine learning} is defined as\cite{DanielFaggella2019} ``Machine Learning is the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.''


\section*{Historic Perspective}

In his excellent blog post \cite{Vazquez2018} Favio V\'{a}zquez gives a brief introduction to the history of \ac{DL} with a very nice historic time line (Figure~\ref{fig:deeplearningtimeline}).

\begin{figure*}[tph]
\includegraphics[width=\textwidth]{deeplearningtimeline}
\caption{Favio V\'{a}zquez's \ac{DL} time line}
\label{fig:deeplearningtimeline}
\end{figure*}



%\FloatBarrier

\section*{The Plan}

My present motivated plan to learn more about \ac{ML} is as shown below. This is the third plan since I started, which demonstrates the volatility of the \ac{ML} world. What seems like a good idea today is in insanely poor choice six months later.

\begin{enumerate}
\item \marginnote{A study \cite{Ericsson93therole} found that full mastery requires long and consistent practice, but another study \cite{Macnamara2014} found that practice alone is not a guarantee for success.} Select the technology/product with utmost care.  Provided you have the ability, it takes ten years or ten thousand hours to fully master a topic. How many ten-year cycles can you afford in your lifetime?

\item \marginnote{Even staying with large companies is risky. Google is notorious for dropping services. Do you remember the MFC technology from Microsoft? It is interesting that Qt, HTML, Python and JavaScript are still around after all these years.} You want to invest in a technology with a long shelf life (probably not possible in \ac{ML}). Stay with mainstream and large projects with a wide following. Stay away from small or risky projects, time spent there is time wasted. 
    
\item I want to stay on the Windows platform this is where all my tools are (I do not have a Linux boot PC at present).  This requirement severely limits my options (see below). Linux is miles ahead here, most of the serious work is done on Linux.
    
\item Stay local, not in the cloud.   If you have large models with large data sets, using the cloud services is far better, but I have no funding to pay for cloud services at this time.

\item Stay with open source projects with large following and a good Internet support base. The bigger the user base, the better you protect your investment in terms of support and long term survivability. Services such as StackOverflow provide more useful support than does commercial companies, simply because of the large support base. People often denigrate the value of YouTube, but there are also some seriously useful tutorials and support information there. 

\item Steps:
\begin{enumerate}
\item Start with Michael Nielsen's excellent introductory online book \cite{Nielsen2015}. His book is included as Part~I of this document.  The purpose with the book is to provide understanding at a conceptual level, not as a rigourous academic treatise; which is what I need to start of with.
\item Start with Scikit Learn, simply because it is easily installable, does not require a \ac{GPU} software installation, has a large example base and is reasonably well documented.
\item Invest in learning TensorFlow \cite{TensorFlow2Alpha2019}.  I previously considered PyTorch \cite{paszkePyTorch2017,PyTorch2019} and fastai \cite{fastai2019}, but found it difficult to do a GPU install on Windows.  One of the objections against TensorFlow~1 was its awkward \lstinline{sessions}  construction, but this falls away with TensorFlow~2.  Google also integrated Keras \cite{cholletkeras2015,cholletkerasio2015} into TensorFlow~2 as a native higher-level framework.
    Installation of GPU TensorFlow~2 is still not simple for Windows, but I will start with the \ac{CPU} version.
\item Work through G\'{e}ron's book \cite{geron2017handson} \textit{Hands-on machine learning with Scikit-Learn and TensorFlow}.  The first part of the book uses Scikit Learn to establish a good understanding and then moves on to using the TensorFlow tools.
    The book receives good reviews and is presently under revision for a second edition, covering TensorFlow~2.
\item I want to document my learning process in Part~II of this book, and add insights from other authors into Part~III of this book. This will come as time availability allows.
\item For in-depth backup and if I ever need this, I will consult the academic books \cite{geron2017handson,Webb2002statpatn,Michie94,theodoridis2003,Duda2001,Bishop1995,Bishop2006,Goodfellow2016}, but for now, my interest is still at an elementary level.
\end{enumerate}
\item All of the above will be done at a leisurely pace outside of working hours and jointly with my life partner and fellow traveller Riana.
\end{enumerate}


\section*{Attribution}

\begin{enumerate}
\item 
The entirety of Part I is taken from Michael Nielsen's online book\cite{Nielsen2015}. Some minor changes were made to the text.

\item
The stylistic neuron on the Part header pages were adapted from \cite{Erler2004}. 

\item
Some of the neural diagrams  are created with the \lstinline{neural} TikZ module for \LaTeX{}\cite{cowan2019} by Mark Cowan.
\end{enumerate}
